{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1a68d9-a1b9-4156-a792-6d182a50f076",
   "metadata": {},
   "source": [
    "# ----------------------------------------------- #\n",
    "# DEBUT\n",
    "# ----------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f128b-7807-4451-a0a2-7602839cd29d",
   "metadata": {},
   "source": [
    "### Diviser les données en ensembles d'entraînement, de validation, et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0341ccf3-aab8-4dbd-8568-5a712ea70c04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T14:57:31.519061Z",
     "iopub.status.busy": "2024-04-25T14:57:31.518047Z",
     "iopub.status.idle": "2024-04-25T14:57:57.716585Z",
     "shell.execute_reply": "2024-04-25T14:57:57.715270Z",
     "shell.execute_reply.started": "2024-04-25T14:57:31.519014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTForImageClassification, AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Détermination du device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Chemin et transformations\n",
    "dataset_path = \"../../../../images/image_train/\"\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "full_dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "\n",
    "\n",
    "# Split\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "test_size = int(0.15 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size - test_size\n",
    "train_dataset, remaining_dataset = random_split(full_dataset, [train_size, len(full_dataset) - train_size])\n",
    "test_dataset, val_dataset = random_split(remaining_dataset, [test_size, val_size])\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9857ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765c14d-fc6b-47be-8ea9-5e82e7ba68af",
   "metadata": {},
   "source": [
    "### Chargement du modèle ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea9f425-012b-431f-aadd-f650060414e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T15:01:29.792856Z",
     "iopub.status.busy": "2024-04-25T15:01:29.791524Z",
     "iopub.status.idle": "2024-04-25T15:01:41.822838Z",
     "shell.execute_reply": "2024-04-25T15:01:41.821997Z",
     "shell.execute_reply.started": "2024-04-25T15:01:29.792824Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modèle, optimizer, early stopping\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=27)\n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92efb70-7e4e-4d89-b00f-9328e5b4680d",
   "metadata": {},
   "source": [
    "### Configuration de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab4b140-48e5-44ae-81b3-6ceab9e60e8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T15:01:41.827493Z",
     "iopub.status.busy": "2024-04-25T15:01:41.827258Z",
     "iopub.status.idle": "2024-04-25T15:01:41.847893Z",
     "shell.execute_reply": "2024-04-25T15:01:41.846490Z",
     "shell.execute_reply.started": "2024-04-25T15:01:41.827467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checkpointing\n",
    "def save_checkpoint(model, optimizer, filename='checkpoint.pth.tar'):\n",
    "    checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, filename)\n",
    "    \n",
    "# Early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=True, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.logits, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    return f1_score(all_labels, all_preds, average='weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255be5eb-5f01-410b-9f6b-13f13207009b",
   "metadata": {},
   "source": [
    "### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c6c6f2-bf41-4a4b-9ae2-b2a00ca26c5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T21:38:48.301344Z",
     "iopub.status.busy": "2024-04-24T21:38:48.300744Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.260935194872428, Validation F1: 0.5042098776342085\n",
      "Checkpoint saved\n",
      "Epoch 2, Loss: 1.5133825115881105, Validation F1: 0.5862536255514254\n",
      "Checkpoint saved\n",
      "Epoch 3, Loss: 1.1851741508815599, Validation F1: 0.6257331723925031\n",
      "Checkpoint saved\n",
      "Epoch 4, Loss: 0.9456648710672406, Validation F1: 0.6363475981911423\n",
      "Checkpoint saved\n",
      "Epoch 5, Loss: 0.7390496625070987, Validation F1: 0.6475178543225203\n",
      "Checkpoint saved\n",
      "Epoch 6, Loss: 0.5579295889622923, Validation F1: 0.6519888364359901\n",
      "Checkpoint saved\n",
      "Epoch 7, Loss: 0.4080011513872423, Validation F1: 0.6464504427605066\n",
      "Epoch 8, Loss: 0.2960020390714424, Validation F1: 0.6473677736487412\n",
      "Epoch 9, Loss: 0.21449461048711901, Validation F1: 0.6457706929084437\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "#Early Stopping\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "\n",
    "# suivi entrainement\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_f1 = 0\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    val_f1 = evaluate(model, val_loader)\n",
    "    writer.add_scalar('Training Loss', running_loss / len(train_loader), epoch)\n",
    "    writer.add_scalar('Validation F1', val_f1, epoch)\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}, Validation F1: {val_f1}')\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        save_checkpoint(model, optimizer, filename=f'checkpoint_epoch_{epoch}.pth.tar')\n",
    "        print(\"Checkpoint saved\")\n",
    "\n",
    "    early_stopping(-val_f1, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51c2ec-e417-40e8-8e6c-ca31c179cbe7",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4817101-47ea-4322-ab09-2655f3e013fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T15:02:50.567385Z",
     "iopub.status.busy": "2024-04-25T15:02:50.566332Z",
     "iopub.status.idle": "2024-04-25T15:02:56.802760Z",
     "shell.execute_reply": "2024-04-25T15:02:56.801830Z",
     "shell.execute_reply.started": "2024-04-25T15:02:50.567355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load('checkpoint_epoch_5.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635d8b11-75ec-4d67-a28d-874a6702c5fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T15:03:06.001515Z",
     "iopub.status.busy": "2024-04-25T15:03:06.000698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Weighted F1 Score: 0.6377361624847758\n"
     ]
    }
   ],
   "source": [
    "# Test Evaluation\n",
    "test_f1 = evaluate(model, test_loader)\n",
    "print(f'Test Weighted F1 Score: {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa537ec-852b-4dbb-b5ec-d0fda5cf48a0",
   "metadata": {},
   "source": [
    "# ----------------------------------------------- #\n",
    "# FIN\n",
    "# ----------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0368225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du checkpoint\n",
    "checkpoint_path = './checkpoint/checkpoint_epoch_5.pth.tar' # Mettez à jour le chemin\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Passer le modèle en mode évaluation\n",
    "model.eval()\n",
    "\n",
    "# Supposons que vous ayez un DataLoader pour lequel vous souhaitez extraire des embeddings\n",
    "embeddings = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:  # Remplacez `test_loader` par votre DataLoader spécifique\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        embeddings.extend(outputs.logits.cpu().numpy())  # Adjust if you need a different output layer\n",
    "        labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# `embeddings` contiendra maintenant les embeddings pour votre ensemble de données\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
